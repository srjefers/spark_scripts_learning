from pyspark.sql import SparkSession, HiveContext

spark = (SparkSession
         .builder
         .appName('exersice_3')
         .getOrCreate())

df = spark.createDataFrame([("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001",  2),
  ("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001",  2),
  ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001",  2),
  ("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001",  2),
  ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001",  2),
  ("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001",  2),
  ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001",  2),
  ("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001",  2),
  ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001",  2),
  ("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001",  2),
  ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001",  2),
  ("05:49:56.604900", "10.0.0.2.54880", "10.0.0.3.5001",  2),
  ("05:49:56.604899", "10.0.0.2.54880", "10.0.0.3.5001",  2),
  ("05:49:56.604908", "10.0.0.3.5001",  "10.0.0.2.54880", 2),
  ("05:49:56.604908", "10.0.0.3.5001",  "10.0.0.2.54880", 2),
  ("05:49:56.604908", "10.0.0.3.5001",  "10.0.0.2.54880", 2),
  ("05:49:56.604908", "10.0.0.3.5001",  "10.0.0.2.54880", 2),
  ("05:49:56.604908", "10.0.0.3.5001",  "10.0.0.2.54880", 2),
  ("05:49:56.604908", "10.0.0.3.5001",  "10.0.0.2.54880", 2),
  ("05:49:56.604908", "10.0.0.3.5001",  "10.0.0.2.54880", 2)],["column0", "column1", "column2", "label"])

df.registerTempTable("dfTable")

df2 = spark.sql("""
                SELECT
                    column0, column1, column2, label,
                    count(1) over (partition by column1, column2, label) cnt
                FROM dfTable
                """)

df2.show()